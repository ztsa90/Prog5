#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Assignment 6 â€” Stage 1
Load the dbNSFP 4.9a dataset into a PySpark DataFrame (sampled),
inspect basic structure, and auto-detect â€œclassifierâ€ columns
by column-name suffix patterns.
"""

import sys
# --- Add Spark libraries before importing SparkSession (BIN/assemblix paths) ---
sys.path.append("/opt/spark/python")
sys.path.append("/opt/spark/python/lib/py4j-0.10.9.7-src.zip")

from pyspark.sql import SparkSession

# ---------------------------------------------------------------------
# 1) Create a local Spark session (no UI; modest memory fits a sample)
# ---------------------------------------------------------------------
spark = (
    SparkSession.builder.appName("assignment6_stage1_Ztaherihanjani")
    .config("spark.ui.enabled", "false")
    .config("spark.executor.memory", "8g")
    .config("spark.driver.memory", "8g")
    .master("local[8]")   # use 8 local cores
    .getOrCreate()
)
spark.sparkContext.setLogLevel("ERROR")

# ---------------------------------------------------------------------
# 2) Load a manageable sample from the compressed TSV
#    (Spark can read .gz transparently; inferSchema=False keeps all as strings)
# ---------------------------------------------------------------------
DATA_PATH = "/data/datasets/dbNSFP/snpEff/data/dbNSFP4.9a.txt.gz"

df = (
    spark.read.csv(
        DATA_PATH,
        sep="\t",
        header=True,
        inferSchema=False
    )
    .limit(2000)  # sample only; avoids scanning 37GB in stage 1
)

# ---------------------------------------------------------------------
# 3) Quick exploration
# ---------------------------------------------------------------------
row_count = df.count()  # action (safe here; limited to 2000)
col_count = len(df.columns)

print(f"âœ… Rows: {row_count:,}")
print(f"âœ… Columns: {col_count}")
print("âœ… First 30 column names:", df.columns[:30])

print("\nðŸ”¹ Sample rows:")
df.show(5, truncate=False)

# ---------------------------------------------------------------------
# 4) Identify columns that belong to classifier outputs
#    Rationale: in dbNSFP, prediction tools expose *_score, *_pred, *_rankscore, *_phred
#    (e.g., SIFT_score, Polyphen2_HDIV_pred, REVEL_rankscore, CADD_phred, ...)
# ---------------------------------------------------------------------
classifier_cols = [
    c for c in df.columns
    if any(suffix in c for suffix in ("_score", "_pred", "_rankscore", "_phred"))
]

print("\nðŸ§© Number of classifier-related columns:", len(classifier_cols))
print("ðŸ§© Example classifier columns:", classifier_cols[:20])

# Extract the *base* classifier tool names (everything before the first underscore)
# Examples:
#   "SIFT_score"              -> "SIFT"
#   "Polyphen2_HDIV_score"    -> "Polyphen2"
#   "fathmm-MKL_coding_score" -> "fathmm-MKL"
classifiers = sorted(set(c.split("_")[0] for c in classifier_cols))

print(f"\nðŸ§ª Distinct classifiers detected: {len(classifiers)}")
print("ðŸ§ª Classifier names (first 30):", classifiers[:30])


# WHITELIST_CLASSIFIERS = {
#     "SIFT", "SIFT4G", "Polyphen2", "LRT", "MutationTaster",
#     "MutationAssessor", "FATHMM", "PROVEAN", "VEST4", "MetaSVM",
#     "MetaLR", "MetaRNN", "M-CAP", "REVEL", "MutPred", "MVP", "gMVP",
#     "MPC", "PrimateAI", "DEOGEN2", "BayesDel", "ClinPred", "LIST-S2",
#     "VARITY_R", "VARITY_ER", "ESM1b", "EVE", "AlphaMissense",
#     "PHACTboost", "MutFormer", "MutScore", "Aloft", "CADD", "DANN",
#     "fathmm-MKL", "fathmm-XF", "Eigen", "GenoCanyon", "LINSIGHT"
# }

# classifiers = [c for c in classifiers if c in WHITELIST_CLASSIFIERS]

spark.stop()
